{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/02\n",
      "Index([u'momentumx', u'momentumy', u'momentumz', u'momentum', u'energy',\n",
      "       u'position', u'MvdDEDX', u'MvdHits', u'SttMeanDEDX', u'SttHits',\n",
      "       u'GemHits', u'TofStopTime', u'TofM2', u'TofTrackLength', u'TofQuality',\n",
      "       u'TofBeta', u'DrcThetaC', u'DrcQuality', u'DiscThetaC', u'DiscQuality',\n",
      "       u'EmcRawEnergy', u'EmcCalEnergy', u'EmcQuality', u'EmcNumberOfCrystals',\n",
      "       u'EmcNumberOfBumps', u'EmcModule', u'EmcZ20', u'EmcZ53', u'EmcLat',\n",
      "       u'EmcE1', u'EmcE9', u'EmcE25', u'MuoQuality', u'MuoIron',\n",
      "       u'MuoMomentumIn', u'MuoNumberOfLayers', u'MuoModule', u'MuoHits',\n",
      "       u'DegreesOfFreedom', u'ChiSquared', u'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/wesmail/Panda/spark-2.2.1-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder  \\\n",
    ".master(\"local[2]\")  \\\n",
    ".appName(\"Linear Regrssion Model\")  \\\n",
    ".config(\"spark.executor.memory\",\"8gb\")  \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# LOAD HERE\n",
    "from root_numpy import root2array, tree2array, rec2array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "branch_names = \"\"\"momentumx, momentumy,momentumz,momentum,energy,position,MvdDEDX,MvdHits,SttMeanDEDX,SttHits,GemHits,TofStopTime,\n",
    "TofM2,TofTrackLength,TofQuality,TofBeta,DrcThetaC,DrcQuality,\n",
    "DiscThetaC,DiscQuality,\n",
    "EmcRawEnergy,EmcCalEnergy,EmcQuality,EmcNumberOfCrystals,EmcNumberOfBumps,EmcModule,\n",
    "EmcZ20,EmcZ53,EmcLat,EmcE1,EmcE9,EmcE25,MuoQuality,MuoIron,MuoMomentumIn,MuoNumberOfLayers,MuoModule,MuoHits,\n",
    "DegreesOfFreedom,ChiSquared\"\"\".split(\",\")\n",
    "branch_names = [c.strip() for c in branch_names]\n",
    "branch_names = list(branch_names)\n",
    "\n",
    "electrons = root2array(\"/home/wesmail/Downloads/treeElectrons.root\", \"t1\", branch_names)\n",
    "electrons = rec2array(electrons)\n",
    "\n",
    "pions = root2array(\"/home/wesmail/Downloads/treePions.root\", \"t1\", branch_names)\n",
    "pions = rec2array(pions)\n",
    "\n",
    "muons = root2array(\"/home/wesmail/Downloads/treeMuons.root\", \"t1\", branch_names)\n",
    "muons = rec2array(muons)\n",
    "\n",
    "kaons = root2array(\"/home/wesmail/Downloads/treeKaons.root\", \"t1\", branch_names)\n",
    "kaons = rec2array(kaons)\n",
    "\n",
    "anti_p = root2array(\"/home/wesmail/Downloads/treeProtons.root\", \"t1\", branch_names)\n",
    "anti_p = rec2array(anti_p)\n",
    "\n",
    "X = np.concatenate((electrons, pions, muons, kaons, anti_p))\n",
    "y = np.concatenate(( np.zeros(electrons.shape[0]), (np.ones(pions.shape[0])), (2*np.ones(muons.shape[0])), (3*np.ones(kaons.shape[0])), (4*np.ones(anti_p.shape[0])) ))\n",
    "\n",
    "# scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create DataFrame from X and y\n",
    "df = pd.DataFrame(np.hstack((X_scaled, y.reshape(y.shape[0], -1))),columns=branch_names+['label'])\n",
    "\n",
    "# compute the mean value per feature on the data set\n",
    "#mean_on_data = (df.iloc[:,0:-1]).mean(axis=0)\n",
    "# compute the standard deviation of each feature on the data set\n",
    "#std_on_data = (df.iloc[:,0:-1]).std(axis=0)\n",
    "# subtract the mean, scale by inverse standard deviation\n",
    "# afterwards, mean=0 and std=1\n",
    "#df_scaled = ((df.iloc[:,0:-1]) - mean_on_data) / std_on_data\n",
    "\n",
    "#df_scaled = pd.DataFrame(array_scaled, columns=branch_names)\n",
    "#df_scaled['label'] = df.loc[:,'label']\n",
    "\n",
    "print(df.columns)\n",
    "# Create Spark DataFrame\n",
    "sparkRDD = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the branches into feature coloumn\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=branch_names,outputCol=\"features\")\n",
    "data = assembler.transform(sparkRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.839176738064\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed=0)\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 44 (features), two intermediate of size 50 and 50\n",
    "# and output of size 5 (classes)\n",
    "layers = [40, 100, 100, 5]\n",
    "\n",
    "# Train a RandomForest model.\n",
    "mlp = MultilayerPerceptronClassifier(labelCol='label', featuresCol='features', maxIter=500, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# Fit model.\n",
    "model = mlp.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[742] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "[[10054.   145.    87.    54.    57.]\n",
      " [  216.  7168.  1192.  1018.   335.]\n",
      " [  124.   712.  9439.   167.    76.]\n",
      " [  136.  1063.   343.  7275.   863.]\n",
      " [   82.   366.    83.   945.  8142.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f2f93b62f50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'predictionAndLabels' into spark RDD\n",
    "mypred = predictionAndLabels.rdd\n",
    "print (mypred)\n",
    "\n",
    "# Evaluation metrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(mypred)\n",
    "\n",
    "# Build the confusion matrix\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "# Show the confusion matrix\n",
    "print (cm)\n",
    "\n",
    "# Normalize and plot the confusion matrix (Rows = True values, Coloumns = Predicted Values)\n",
    "# normalized over coloumns (axis=1)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "figcm, ax = plt.subplots()\n",
    "cm = cm.astype('float') / cm.sum(axis=1)\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(cm, square=True, annot=True, cbar=False)\n",
    "classes=['e-','pi-', 'mu-', 'k-', 'p-']    # 0 1 2 3 4 \n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "ax.set_xticks(np.arange(len(classes))+0.5, minor=False)\n",
    "plt.yticks(tick_marks, classes)\n",
    "ax.set_yticks(np.arange(len(classes))+0.5, minor=False)\n",
    "plt.xlabel('predication', horizontalalignment = 'center')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
